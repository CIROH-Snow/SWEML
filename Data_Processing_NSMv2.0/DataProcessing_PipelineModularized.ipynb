{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582b19b-a266-4a82-9f9d-7d5ad1e3a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "# Dataframe Packages\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Vector Packages\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Point, Polygon\n",
    "from pyproj import CRS, Transformer\n",
    "\n",
    "# Raster Packages\n",
    "import rioxarray as rxr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from rioxarray.merge import merge_arrays\n",
    "import rasterstats as rs\n",
    "import osgeo\n",
    "from osgeo import gdalconst\n",
    "\n",
    "# Data Access Packages\n",
    "import earthaccess as ea\n",
    "import h5py\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "from pystac_client import Client\n",
    "import richdem as rd\n",
    "import planetary_computer\n",
    "from planetary_computer import sign\n",
    "\n",
    "# General Packages\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import math\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from pprint import pprint\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import progress\n",
    "from dask.distributed import Client\n",
    "from dask.diagnostics import ProgressBar\n",
    "from retrying import retry\n",
    "import fiona\n",
    "import re\n",
    "import s3fs\n",
    "\n",
    "#need to mamba install gdal, earthaccess \n",
    "#pip install pystac_client, richdem, planetary_computer, dask, distributed, retrying\n",
    "\n",
    "#connecting to AWS\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "\n",
    "import NSIDC_Data\n",
    "'''\n",
    "To create .netrc file:\n",
    "import earthaccess\n",
    "earthaccess.login(persist=True)\n",
    "open file and change machine to https://urs.earthdata.nasa.gov\n",
    "\n",
    "'''\n",
    "\n",
    "#load access key\n",
    "HOME = os.path.expanduser('~')\n",
    "KEYPATH = \"SWEML/AWSaccessKeys.csv\"\n",
    "ACCESS = pd.read_csv(f\"{HOME}/{KEYPATH}\")\n",
    "\n",
    "#start session\n",
    "SESSION = boto3.Session(\n",
    "    aws_access_key_id=ACCESS['Access key ID'][0],\n",
    "    aws_secret_access_key=ACCESS['Secret access key'][0],\n",
    ")\n",
    "S3 = SESSION.resource('s3')\n",
    "#AWS BUCKET information\n",
    "BUCKET_NAME = 'national-snow-model'\n",
    "#S3 = boto3.resource('S3', config=Config(signature_version=UNSIGNED))\n",
    "BUCKET = S3.Bucket(BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASOget import ASODataTool, ASODownload\n",
    "\n",
    "#Get ASO data for region\n",
    "short_name = 'ASO_50M_SWE'\n",
    "version = '1'\n",
    "\n",
    "data_tool = ASODownload(short_name, version)\n",
    "time_start = '2013-04-02T00:00:00Z'\n",
    "time_end = '2019-07-19T23:59:59Z'\n",
    "\n",
    "selected_region = data_tool.select_region()  # Call select_region on the instance, S Sierras is #2. There may be a need to modify this in order to cover CONUS, create region folders...\n",
    "directory = \"SWE_Data\"\n",
    "\n",
    "print(f\"Fetching file URLs in progress for {selected_region} from {time_start} to {time_end}\")\n",
    "url_list = data_tool.cmr_search(time_start, time_end, data_tool.bounding_box)\n",
    "data_tool.cmr_download(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0aa682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjohnson18/envs/SWEML_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-18 11:52:26.999546: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-18 11:52:26.999568: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 131 ASO tif files to csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [02:36<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "from ASOget import ASODataProcessing\n",
    "\n",
    "data_processor = ASODataProcessing()\n",
    "folder_name = \"SWE_Data\"\n",
    "output_res = 100 #0.001 changed to make more interpretable, are we sure this current number is correct? I would think that is 1000m or 1km...\n",
    "data_processor.convert_tiff_to_csv(folder_name, output_res) #Takes ~3 mins, can it be multithreaded?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a057f236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjohnson18/envs/SWEML_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-18 12:17:01.480806: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-18 12:17:01.480827: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying polygon geometries, please be patient, this step can take a few minutes...\n",
      "Converting to GeoDataFrame\n",
      "Processing 99 for dataframe development\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [12:14<00:00,  7.42s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_folder = f\"ASO/100M_SWE_csv\"\n",
    "metadata_path = f\"TrainingDFs/grid_cells_meta.csv\"\n",
    "output_folder = f\"Processed_SWE\"\n",
    "\n",
    "data_processor = ASODataProcessing()#run if not ran in code block above...Takes ~20 minutes. Can this be multithreaded?\n",
    "data_processor.process_folder(input_folder, metadata_path, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7486cc31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c91c7a-d327-4dbf-8669-1e667c0e4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_aso_snotel_geometry(aso_swe_file, folder_path):\n",
    "    \n",
    "    aso_file = pd.read_csv(os.path.join(folder_path, aso_swe_file))\n",
    "    aso_file.set_index('cell_id', inplace=True)\n",
    "    aso_geometry = [Point(xy) for xy in zip(aso_file['x'], aso_file['y'])]\n",
    "    aso_gdf = gpd.GeoDataFrame(aso_file, geometry=aso_geometry)\n",
    "    \n",
    "    return aso_gdf\n",
    "\n",
    "def haversine_vectorized(lat1, lon1, lat2, lon2):\n",
    "    \n",
    "    lon1 = np.radians(lon1)\n",
    "    lon2 = np.radians(lon2)\n",
    "    lat1 = np.radians(lat1)\n",
    "    lat2 = np.radians(lat2)\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    r = 6371.0\n",
    "    # Distance calculation\n",
    "    distances = r * c\n",
    "\n",
    "    return distances\n",
    "\n",
    "def calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6, distance_cache=None):\n",
    "\n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    for idx, aso_row in aso_gdf.iterrows():\n",
    "        cell_id = idx\n",
    "\n",
    "        # Check if distances for this cell_id are already calculated and cached\n",
    "        if cell_id in distance_cache:\n",
    "            nearest_snotel[idx] = distance_cache[cell_id]\n",
    "        else:\n",
    "            # Calculate Haversine distances between the grid cell and all SNOTEL locations\n",
    "            distances = haversine_vectorized(\n",
    "                aso_row.geometry.y, aso_row.geometry.x,\n",
    "                snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "\n",
    "            # Store the nearest stations in the cache\n",
    "            nearest_snotel[idx] = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "            distance_cache[cell_id] = nearest_snotel[idx]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def calculate_distances_for_cell(aso_row, snotel_gdf, n=6):\n",
    "   \n",
    "    distances = haversine_vectorized(\n",
    "        aso_row.geometry.y, aso_row.geometry.x,\n",
    "        snotel_gdf.geometry.y.values, snotel_gdf.geometry.x.values)\n",
    "    \n",
    "    nearest_sites = list(snotel_gdf['station_id'].iloc[distances.argsort()[:n]])\n",
    "    \n",
    "    return nearest_sites\n",
    "\n",
    "def calculate_nearest_snotel_parallel(aso_gdf, snotel_gdf, n = 6, distance_cache = None):\n",
    "    \n",
    "    if distance_cache is None:\n",
    "        distance_cache = {}\n",
    "\n",
    "    nearest_snotel = {}\n",
    "    with ProcessPoolExecutor(max_workers = 16) as executor:\n",
    "        futures = []\n",
    "        \n",
    "        for idx, aso_row in aso_gdf.iterrows():\n",
    "            if idx not in distance_cache:\n",
    "                # Submit the task for parallel execution\n",
    "                futures.append(executor.submit(calculate_distances_for_cell, aso_row, snotel_gdf, n))\n",
    "            else:\n",
    "                nearest_snotel[idx] = distance_cache[idx]\n",
    "\n",
    "        # Retrieve results as they are completed\n",
    "        for future in tqdm(futures):\n",
    "            result = future.result()\n",
    "  \n",
    "            cell_id = result[0]  \n",
    "            nearest_snotel[cell_id] = result[1]\n",
    "            distance_cache[cell_id] = result[1]\n",
    "\n",
    "    return nearest_snotel, distance_cache\n",
    "\n",
    "def fetch_snotel_sites_for_cellids(aso_swe_files_folder_path, metadata_path, snotel_data_path):\n",
    "    \n",
    "    metadata_df = pd.read_csv(metadata_path)\n",
    "    #metadata_df['geometry'] = metadata_df['geometry'].apply(wkt.loads)\n",
    "    \n",
    "    def create_polygon(row):\n",
    "        return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                        (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                        (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                        (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "        \n",
    "    metadata_df = metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    metadata_df['geometry'] = metadata_df.apply(create_polygon, axis=1)\n",
    "    \n",
    "    metadata = gpd.GeoDataFrame(metadata_df, geometry='geometry')\n",
    "    snotel_data = pd.read_csv(snotel_data_path)\n",
    "\n",
    "    date_columns = snotel_data.columns[1:]\n",
    "    new_column_names = {col: pd.to_datetime(col, format='%Y-%m-%d').strftime('%Y%m%d') for col in date_columns}\n",
    "    snotel_data_f = snotel_data.rename(columns=new_column_names)\n",
    "\n",
    "    snotel_file = pd.read_csv(\"/home/vgindi/Provided_Data/ground_measures_metadata.csv\")\n",
    "    snotel_geometry = [Point(xy) for xy in zip(snotel_file['longitude'], snotel_file['latitude'])]\n",
    "    snotel_gdf = gpd.GeoDataFrame(snotel_file, geometry=snotel_geometry)\n",
    "\n",
    "    final_df = pd.DataFrame()\n",
    "\n",
    "    for aso_swe_file in os.listdir(aso_swe_files_folder_path):\n",
    "\n",
    "        if os.path.isdir(os.path.join(aso_swe_files_folder_path, aso_swe_file)):\n",
    "            continue\n",
    "\n",
    "        timestamp = aso_swe_file.split('_')[-1].split('.')[0]\n",
    "        print(f\"Processing file with timestamp: {timestamp}\")\n",
    "\n",
    "        aso_gdf = load_aso_snotel_geometry(aso_swe_file, aso_swe_files_folder_path)\n",
    "        aso_swe_data = pd.read_csv(os.path.join(aso_swe_files_folder_path, aso_swe_file))\n",
    "\n",
    "        # Calculating nearest SNOTEL sites\n",
    "        nearest_snotel, distance_cache = calculate_nearest_snotel(aso_gdf, snotel_gdf, n=6)\n",
    "        print(f\"calculated nearest snotel for file with timestamp {timestamp}\")\n",
    "\n",
    "        transposed_data = {}\n",
    "\n",
    "        if timestamp in new_column_names.values():\n",
    "            for idx, aso_row in aso_gdf.iterrows():    \n",
    "                cell_id = idx\n",
    "                station_ids = nearest_snotel[cell_id]\n",
    "                selected_snotel_data = snotel_data_f[['station_id', timestamp]].loc[snotel_data_f['station_id'].isin(station_ids)]\n",
    "                station_mapping = {old_id: f\"nearest site {i+1}\" for i, old_id in enumerate(station_ids)}\n",
    "                \n",
    "                # Rename the station IDs in the selected SNOTEL data\n",
    "                selected_snotel_data['station_id'] = selected_snotel_data['station_id'].map(station_mapping)\n",
    "\n",
    "                # Transpose and set the index correctly\n",
    "                transposed_data[cell_id] = selected_snotel_data.set_index('station_id').T\n",
    "\n",
    "            transposed_df = pd.concat(transposed_data, axis=0)\n",
    "            \n",
    "            # Reset index and rename columns\n",
    "            transposed_df = transposed_df.reset_index()\n",
    "            transposed_df.rename(columns={'level_0': 'cell_id', 'level_1': 'Date'}, inplace = True)\n",
    "            transposed_df['Date'] = pd.to_datetime(transposed_df['Date'])\n",
    "        \n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "            merged_df = pd.merge(aso_swe_data, transposed_df, how='left', on=['cell_id', 'Date'])\n",
    "        \n",
    "            final_df = pd.concat([final_df, merged_df], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            aso_swe_data['Date'] = pd.to_datetime(timestamp)\n",
    "            aso_swe_data = aso_swe_data[['cell_id', 'Date', 'swe']]\n",
    "    \n",
    "            # No need to merge in this case, directly concatenate\n",
    "            final_df = pd.concat([final_df, aso_swe_data], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Merge with metadata\n",
    "    req_cols = ['cell_id', 'lat', 'lon', 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat', 'geometry']\n",
    "    Result = final_df.merge(metadata[req_cols], how='left', on='cell_id')\n",
    "\n",
    "    # Column renaming and ordering\n",
    "    Result.rename(columns={'swe': 'ASO_SWE_in'}, inplace=True)\n",
    "    Result = Result[['cell_id', 'Date', 'ASO_SWE_in', 'lat', 'lon', 'nearest site 1', 'nearest site 2',\n",
    "                     'nearest site 3', 'nearest site 4', 'nearest site 5', 'nearest site 6',\n",
    "                     'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "                     'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']]\n",
    "\n",
    "    # Save the merged data to a new file\n",
    "    output_filename = r\"/home/vgindi/Provided_Data/Merged_aso_snotel_data.csv\"\n",
    "    Result.to_csv(output_filename, index=False)\n",
    "    print(\"Processed and saved data\")\n",
    "    \n",
    "def main():\n",
    "    aso_swe_files_folder_path = r\"/home/vgindi/Processed_SWE\"\n",
    "    metadata_path = r\"/home/vgindi/Provided_Data/grid_cells_meta_idx.csv\"\n",
    "    snotel_data_path = r\"/home/vgindi/Provided_Data/ground_measures_train_featuresALLDATES.parquet\"\n",
    "    fetch_snotel_sites_for_cellids(aso_swe_files_folder_path, metadata_path, snotel_data_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c69770-62f8-4f8f-88ee-ffaac26aaa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pd.read_csv(r'/home/vgindi/Provided_Data/Merged_aso_snotel_data.csv')\n",
    "Result.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cb0f3a-7713-45d2-881f-574037b3a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A Simple implementation of parallel processing using concurrency it takes so long to execute,\n",
    "Explore terrain_daskconcurrency and terrain-processing_cluster python for more optimized implementations.\n",
    "\"\"\"\n",
    "\n",
    "def process_single_location(args):\n",
    "    lat, lon, regions, tiles = args\n",
    "\n",
    "    if (lat, lon) in elevation_cache:\n",
    "        elev, slop, asp = elevation_cache[(lat, lon)]\n",
    "        return elev, slop, asp\n",
    "\n",
    "    tile_id = 'Copernicus_DSM_COG_30_N' + str(math.floor(lon)) + '_00_W' + str(math.ceil(abs(lat))) + '_00_DEM'\n",
    "    index_id = regions.loc[tile_id]['sliceID']\n",
    "\n",
    "    signed_asset = planetary_computer.sign(tiles[index_id].assets[\"data\"])\n",
    "    #print(signed_asset)\n",
    "    elevation = rxr.open_rasterio(signed_asset.href)\n",
    "    \n",
    "    slope = elevation.copy()\n",
    "    aspect = elevation.copy()\n",
    "\n",
    "    transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "    xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "    tilearray = np.around(elevation.values[0]).astype(int)\n",
    "    #print(tilearray)\n",
    "    geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "\n",
    "    no_data_value = -9999\n",
    "    driver = gdal.GetDriverByName('MEM')\n",
    "    temp_ds = driver.Create('', tilearray.shape[1], tilearray.shape[0], 1, gdalconst.GDT_Float32)\n",
    "\n",
    "    temp_ds.GetRasterBand(1).WriteArray(tilearray)\n",
    "    temp_ds.GetRasterBand(1).SetNoDataValue(no_data_value)\n",
    "    temp_ds.SetProjection('EPSG:4326')\n",
    "    temp_ds.SetGeoTransform(geo)\n",
    "\n",
    "    tilearray_np = temp_ds.GetRasterBand(1).ReadAsArray()\n",
    "    slope_arr, aspect_arr = np.gradient(tilearray_np)\n",
    "    aspect_arr = np.rad2deg(np.arctan2(aspect_arr[0], aspect_arr[1]))\n",
    "    \n",
    "    slope.values[0] = slope_arr\n",
    "    aspect.values[0] = aspect_arr\n",
    "\n",
    "    elev = round(elevation.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    slop = round(slope.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "    asp = round(aspect.sel(x=xx, y=yy, method=\"nearest\").values[0])\n",
    "\n",
    "    elevation_cache[(lat, lon)] = (elev, slop, asp)  \n",
    "    return elev, slop, asp\n",
    "\n",
    "def extract_terrain_data_threaded(metadata_df, bounding_box, max_workers=10):\n",
    "    global elevation_cache \n",
    "\n",
    "    elevation_cache = {} \n",
    "    min_x, min_y, max_x, max_y = *bounding_box[0], *bounding_box[1]\n",
    "    \n",
    "    client = Client.open(\n",
    "            \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "            ignore_conformance=True,\n",
    "        )\n",
    "\n",
    "    search = client.search(\n",
    "                    collections=[\"cop-dem-glo-90\"],\n",
    "                    intersects = {\n",
    "                            \"type\": \"Polygon\",\n",
    "                            \"coordinates\": [[\n",
    "                            [min_x, min_y],\n",
    "                            [max_x, min_y],\n",
    "                            [max_x, max_y],\n",
    "                            [min_x, max_y],\n",
    "                            [min_x, min_y]  \n",
    "                        ]]})\n",
    "\n",
    "    tiles = list(search.items())\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    print(\"Retrieving Copernicus 90m DEM tiles\")\n",
    "    for i in tqdm(range(0, len(tiles))):\n",
    "        row = [i, tiles[i].id]\n",
    "        regions.append(row)\n",
    "    regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "    regions = regions.set_index(regions['tileID'])\n",
    "    del regions['tileID']\n",
    "\n",
    "    print(\"Interpolating Grid Cell Spatial Features\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_single_location, (metadata_df.iloc[i]['cen_lat'], metadata_df.iloc[i]['cen_lon'], regions, tiles))\n",
    "                   for i in tqdm(range(len(metadata_df)))]\n",
    "        \n",
    "        results = []\n",
    "        for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "            results.append(future.result())\n",
    "    \n",
    "    metadata_df['Elevation_m'], metadata_df['Slope_Deg'], metadata_df['Aspect_L'] = zip(*results)\n",
    "\n",
    "metadata_df = pd.read_csv(r\"/home/vgindi/Provided_Data/Merged_aso_nearest_sites1.csv\")\n",
    "metadata_df= metadata_df.head(20)\n",
    "bounding_box = ((-120.3763448720203, 36.29256774541929), (-118.292253412863, 38.994985247736324))    \n",
    "    \n",
    "extract_terrain_data_threaded(metadata_df, bounding_box)\n",
    "\n",
    "# Display the results\n",
    "metadata_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f714b0f0-1c38-4ba3-8aed-1ca6b97c2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code block crops the global coverage VIIRS data to south sierras subregion. \n",
    "\"\"\"\n",
    "\n",
    "def crop_sierras(input_file_path, output_file_path, shapes):\n",
    "    with rasterio.open(input_file_path) as src:\n",
    "        out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True)\n",
    "        out_meta = src.out_meta\n",
    "        out_meta.update({\"driver\": \"GTiff\",\n",
    "                         \"height\": out_image.shape[1],\n",
    "                         \"width\": out_image.shape[2],\n",
    "                         \"transform\": out_transform})\n",
    "                         \n",
    "        with rasterio.open(output_file_path, \"w\", **out_meta) as dest:\n",
    "            dest.write(out_image)\n",
    "\n",
    "def download_viirs_sca(input_dir, output_dir, shapefile_path):\n",
    "    \n",
    "    # Load shapes from the shapefile\n",
    "    with fiona.open(shapefile_path, 'r') as shapefile:\n",
    "        shapes = [feature[\"geometry\"] for feature in shapefile]\n",
    "    \n",
    "    # Iterate through each year directory in the input directory\n",
    "    for year_folder in os.listdir(input_dir):\n",
    "        year_folder_path = os.path.join(input_dir, year_folder)\n",
    "        if os.path.isdir(year_folder_path):\n",
    "            # Extract year from the folder name (assuming folder names like 'WY2013')\n",
    "            year = re.search(r'\\d{4}', year_folder).group()\n",
    "            output_year_folder = os.path.join(output_dir, year)\n",
    "            os.makedirs(output_year_folder, exist_ok=True)\n",
    "        \n",
    "            for file_name in os.listdir(year_folder_path):        \n",
    "                if file_name.endswith('.tif'):   \n",
    "                    parts = file_name.split('_')\n",
    "                    output_file_name = '_'.join(parts[:3]) + '.tif'\n",
    "                    output_file_path = os.path.join(output_year_folder, output_file_name)\n",
    "                    input_file_path = os.path.join(year_folder_path, file_name)\n",
    "                    crop_sierras(input_file_path, output_file_path, shapes)\n",
    "                    print(f\"Processed and saved {output_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    input_directory = r\"/home/vgindi/VIIRS_Data\"\n",
    "    output_directory = r\"/home/vgindi/VIIRS_Sierras\"\n",
    "    shapefile_path = r\"/home/vgindi/Provided_Data/low_sierras_points.shp\"\n",
    "    download_viirs_sca(input_directory, output_directory, shapefile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ab2744-b080-48c8-bb77-f4b9c14ca774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell transforms the raw VIIRS tiff files to 100m resolution and saves each file in .csv format\n",
    "\"\"\"\n",
    "def processing_VIIRS(input_file, output_res):\n",
    "    try:\n",
    "        # Define the output file path for TIFFs using the original file name\n",
    "        output_folder_tiff = os.path.join(\"/home/vgindi/Processed_VIIRS\", os.path.basename(os.path.dirname(input_file)))\n",
    "        os.makedirs(output_folder_tiff, exist_ok=True)\n",
    "        output_file = os.path.join(output_folder_tiff, os.path.basename(input_file))\n",
    "\n",
    "        # Reproject and resample\n",
    "        ds = gdal.Open(input_file)\n",
    "        if ds is None:\n",
    "            print(f\"Failed to open '{input_file}'. Make sure the file is a valid GeoTIFF file.\")\n",
    "            return None\n",
    "        \n",
    "        gdal.Warp(output_file, ds, dstSRS=\"EPSG:4326\", xRes=output_res, yRes=-output_res, resampleAlg=\"bilinear\")\n",
    "\n",
    "        # Read the processed TIFF file using rasterio\n",
    "        rds = rxr.open_rasterio(output_file)\n",
    "        rds = rds.squeeze().drop(\"spatial_ref\").drop(\"band\")\n",
    "        rds.name = \"data\"\n",
    "        df = rds.to_dataframe().reset_index()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_and_convert_viirs(input_dir, output_res):\n",
    "    # Iterate over subdirectories in the input directory\n",
    "    for year in os.listdir(input_dir):\n",
    "        year_dir = os.path.join(input_dir, year)\n",
    "        \n",
    "        if os.path.isdir(year_dir):\n",
    "            for file_name in os.listdir(year_dir):\n",
    "                if file_name.endswith('.tif'):\n",
    "                    input_file_path = os.path.join(year_dir, file_name)\n",
    "                    df = processing_VIIRS(input_file_path, output_res)\n",
    "                    \n",
    "                    if df is not None:\n",
    "                        csv_folder = os.path.join(\"/home/vgindi/Processed_VIIRS\", \"VIIRS_csv\")\n",
    "                        os.makedirs(csv_folder, exist_ok=True)\n",
    "                        csv_file_path = os.path.join(csv_folder, file_name.replace('.tif', '.csv'))\n",
    " \n",
    "                        df.to_csv(csv_file_path, index=False)\n",
    "                        print(f\"Processed and saved {csv_file_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_directory = \"/home/vgindi/VIIRS_Sierras\"\n",
    "    output_res = 100  # Desired resolution in meters\n",
    "    process_and_convert_viirs(input_directory, output_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e2831c-817d-40b5-a2e0-d9ebff8a5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code cell fetches the cell id using grid_cells_meta_idx metadata for each lat/lon pair for VIIRS csv file\n",
    "\"\"\"\n",
    "def create_polygon(self, row):\n",
    "    return Polygon([(row['BL_Coord_Long'], row['BL_Coord_Lat']),\n",
    "                    (row['BR_Coord_Long'], row['BR_Coord_Lat']),\n",
    "                    (row['UR_Coord_Long'], row['UR_Coord_Lat']),\n",
    "                    (row['UL_Coord_Long'], row['UL_Coord_Lat'])])\n",
    "    \n",
    "def process_folder(self, input_folder, metadata_path, output_folder):\n",
    "    # Import the metadata into a pandas DataFrame\n",
    "    pred_obs_metadata_df = pd.read_csv(metadata_path)\n",
    "\n",
    "    # Assuming create_polygon is defined elsewhere, we add a column with polygon geometries\n",
    "    pred_obs_metadata_df = pred_obs_metadata_df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "    pred_obs_metadata_df['geometry'] = pred_obs_metadata_df.apply(self.create_polygon, axis=1)\n",
    "\n",
    "    # Convert the DataFrame to a GeoDataFrame\n",
    "    metadata = gpd.GeoDataFrame(pred_obs_metadata_df, geometry='geometry')\n",
    "\n",
    "    # Drop coordinates columns\n",
    "    metadata = metadata.drop(columns=['BL_Coord_Long', 'BL_Coord_Lat', \n",
    "                                         'BR_Coord_Long', 'BR_Coord_Lat', \n",
    "                                         'UR_Coord_Long', 'UR_Coord_Lat', \n",
    "                                         'UL_Coord_Long', 'UL_Coord_Lat'], axis=1)\n",
    "\n",
    "    # List all CSV files in the input folder\n",
    "    csv_files = [f for f in os.listdir(input_folder) if f.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        input_path = os.path.join(input_folder, csv_file)\n",
    "        output_path = os.path.join(output_folder, csv_file)\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        if os.path.exists(output_path):\n",
    "            print(f\"CSV file {csv_file} already exists in the output folder.\")\n",
    "            continue\n",
    "\n",
    "        # Process each CSV file\n",
    "        viirs_sca_df = pd.read_csv(input_path)\n",
    "\n",
    "        # Convert the \"aso_swe_df\" into a GeoDataFrame with point geometries\n",
    "        geometry = [Point(xy) for xy in zip(viirs_sca_df['x'], viirs_sca_df['y'])]\n",
    "        viirs_sca_geo = gpd.GeoDataFrame(viirs_sca_df, geometry=geometry)\n",
    "        result = gpd.sjoin(viirs_sca_geo, metadata, how='left', predicate='within', op = 'intersects')\n",
    "\n",
    "        # Select specific columns for the final DataFrame\n",
    "        Final_df = result[['y', 'x', 'data', 'cell_id']]\n",
    "        Final_df.rename(columns={'data': 'VIIRS_SCA'}, inplace=True)\n",
    "\n",
    "        # Drop rows where 'cell_id' is NaN\n",
    "        if Final_df['cell_id'].isnull().values.any():\n",
    "            Final_df = Final_df.dropna(subset=['cell_id'])\n",
    "\n",
    "        # Save the processed DataFrame to a CSV file\n",
    "        Final_df.to_csv(output_path, index=False)\n",
    "        print(f\"Processed {csv_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = r\"\"\n",
    "    metadata_path = r\"\"\n",
    "    output_folder = r\"\"\n",
    "    process_folder(input_folder, metadata_path, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
