{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc09993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import tables\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import rioxarray as rxr\n",
    "import math\n",
    "import pickle \n",
    "import sklearn\n",
    "import graphviz\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from pickle import dump\n",
    "\n",
    "import contextily as cx\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from xgboost import cv\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance as plot_importance_XGB\n",
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import plot_importance as plot_importance_LGBM\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.utils import np_utils\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from platform import python_version\n",
    "\n",
    "import re\n",
    "\n",
    "def atof(text):\n",
    "    try:\n",
    "        retval = float(text)\n",
    "    except ValueError:\n",
    "        retval = text\n",
    "    return retval\n",
    "\n",
    "def natural_keys(text):\n",
    "    '''\n",
    "    alist.sort(key=natural_keys) sorts in human order\n",
    "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
    "    (See Toothy's implementation in the comments)\n",
    "    float regex comes from https://stackoverflow.com/a/12643073/190597\n",
    "    '''\n",
    "    return [ atof(c) for c in re.split(r'[+-]?([0-9]+(?:[.][0-9]*)?|[.][0-9]+)', text) ]\n",
    "\n",
    "\n",
    "print(pd.__version__) # should be 1.3.0\n",
    "print(sklearn.__version__) # should be 0.24.1\n",
    "print(tf.__version__) # should be 2.4.0\n",
    "print(python_version()) #should be 3.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544a3a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Region identifier. The data already includes Region, but too many 'other' labels\n",
    "\n",
    "def Region_id(df):\n",
    "    \n",
    "    for i in tqdm(range(0, len(df))):\n",
    "\n",
    "        #Sierras\n",
    "        #Northern Sierras\n",
    "        if -122.5 <= df['Long'][i] <=-119 and 39 <=df['Lat'][i] <= 42:\n",
    "            loc = 'N_Sierras'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "        #Southern Sierras\n",
    "        if -121.2 <= df['Long'][i] <=-117 and 35 <=df['Lat'][i] <= 39:\n",
    "            loc = 'S_Sierras'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #West Coast    \n",
    "        #CACoastal (Ca-Or boarder)\n",
    "        if df['Long'][i] <=-122.5 and df['Lat'][i] <= 42:\n",
    "            loc = 'Ca_Coast'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Oregon Coastal (Or)?\n",
    "        if df['Long'][i] <=-122.7 and 42<= df['Lat'][i] <= 46:\n",
    "            loc = 'Or_Coast'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Olympis Coastal (Wa)\n",
    "        if df['Long'][i] <=-122.5 and 46<= df['Lat'][i]:\n",
    "            loc = 'Wa_Coast'\n",
    "            df['Region'].iloc[i] = loc    \n",
    "\n",
    "\n",
    "\n",
    "        #Cascades    \n",
    "         #Northern Cascades\n",
    "        if -122.5 <= df['Long'][i] <=-119.4 and 46 <=df['Lat'][i]:\n",
    "            loc = 'N_Cascade'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Southern Cascades\n",
    "        if -122.7 <= df['Long'][i] <=-121 and 42 <=df['Lat'][i] <= 46:\n",
    "            loc = 'S_Cascade'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Eastern Cascades and Northern Idaho and Western Montana\n",
    "        if -119.4 <= df['Long'][i] <=-116.4 and 46 <=df['Lat'][i]:\n",
    "            loc = 'E_WA_N_Id_W_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "        #Eastern Cascades and Northern Idaho and Western Montana\n",
    "        if -116.4 <= df['Long'][i] <=-114.1 and 46.6 <=df['Lat'][i]:\n",
    "            loc = 'E_WA_N_Id_W_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Eastern Oregon\n",
    "        if -121 <= df['Long'][i] <=-116.4 and 43.5 <=df['Lat'][i] <= 46:\n",
    "            loc = 'E_Or'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Great Basin\n",
    "        if -121 <= df['Long'][i] <=-112 and 42 <=df['Lat'][i] <= 43.5:\n",
    "            loc = 'GBasin'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "        if -119 <= df['Long'][i] <=-112 and 39 <=df['Lat'][i] <= 42:\n",
    "            loc = 'GBasin'\n",
    "            df['Region'].iloc[i] = loc\n",
    "            #note this section includes mojave too\n",
    "        if -117 <= df['Long'][i] <=-113.2 and df['Lat'][i] <= 39:\n",
    "            loc = 'GBasin'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "        #SW Mtns (Az and Nm)\n",
    "        if -113.2 <= df['Long'][i] <=-107 and df['Lat'][i] <= 37:\n",
    "            loc = 'SW_Mtns'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Southern Wasatch + Utah Desert Peaks\n",
    "        if -113.2 <= df['Long'][i] <=-109 and 37 <= df['Lat'][i] <= 39:\n",
    "            loc = 'S_Wasatch'\n",
    "            df['Region'].iloc[i] = loc\n",
    "        #Southern Wasatch + Utah Desert Peaks\n",
    "        if -112 <= df['Long'][i] <=-109 and 39 <= df['Lat'][i] <= 40:\n",
    "            loc = 'S_Wasatch'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Northern Wasatch + Bear River Drainage\n",
    "        if -112 <= df['Long'][i] <=-109 and 40 <= df['Lat'][i] <= 42.5:\n",
    "            loc = 'N_Wasatch'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #YellowStone, Winds, Big horns\n",
    "        if -111 <= df['Long'][i] <=-106.5 and 42.5 <= df['Lat'][i] <= 45.8:\n",
    "            loc = 'Greater_Yellowstone'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #North of YellowStone to Boarder\n",
    "        if -112.5 <= df['Long'][i] <=-106.5 and 45.8 <= df['Lat'][i]:\n",
    "            loc = 'N_Yellowstone'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "         #SW Montana and nearby Idaho\n",
    "        if -112 <= df['Long'][i] <=-111 and 42.5 <= df['Lat'][i] <=45.8:\n",
    "            loc = 'SW_Mont'\n",
    "            df['Region'].iloc[i] = loc \n",
    "         #SW Montana and nearby Idaho\n",
    "        if -113 <= df['Long'][i] <=-112 and 43.5 <= df['Lat'][i] <=45.8:\n",
    "            loc = 'SW_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "        #SW Montana and nearby Idaho\n",
    "        if -113 <= df['Long'][i] <=-112.5 and 45.8 <= df['Lat'][i] <=46.6:\n",
    "            loc = 'SW_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "         #Sawtooths, Idaho\n",
    "        if -116.4 <= df['Long'][i] <=-113 and 43.5 <= df['Lat'][i] <=46.6:\n",
    "            loc = 'Sawtooth'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Greater Glacier\n",
    "        if -114.1 <= df['Long'][i] <=-112.5 and 46.6 <= df['Lat'][i]:\n",
    "            loc = 'Greater_Glacier'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "         #Southern Wyoming \n",
    "        if -109 <= df['Long'][i] <=-104.5 and 40.99 <= df['Lat'][i] <= 42.5 :\n",
    "            loc = 'S_Wyoming'\n",
    "            df['Region'].iloc[i] = loc \n",
    "        #Southern Wyoming\n",
    "        if -106.5 <= df['Long'][i] <=-104.5 and 42.5 <= df['Lat'][i] <= 43.2:\n",
    "            loc = 'S_Wyoming'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         #Northern Colorado Rockies\n",
    "        if -109 <= df['Long'][i] <=-104.5 and 38.3 <= df['Lat'][i] <= 40.99:\n",
    "            loc = 'N_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "         #SW Colorado Rockies\n",
    "        if -109 <= df['Long'][i] <=-106 and 36.99 <= df['Lat'][i] <= 38.3:\n",
    "            loc = 'SW_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #SE Colorado Rockies + Northern New Mexico\n",
    "        if -106 <= df['Long'][i] <=-104.5 and 34 <= df['Lat'][i] <= 38.3:\n",
    "            loc = 'SE_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc  \n",
    "        #SE Colorado Rockies + Northern New Mexico\n",
    "        if -107 <= df['Long'][i] <=-106 and 34 <= df['Lat'][i] <= 36.99:\n",
    "            loc = 'SE_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_num(df):\n",
    "        #week of water year\n",
    "    weeklist = []\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        if df['Date'][i].month<11:\n",
    "            y = df['Date'][i].year-1\n",
    "        else:\n",
    "            y = df['Date'][i].year\n",
    "            \n",
    "        WY_start = pd.to_datetime(str(y)+'-10-01')\n",
    "        deltaday = df['Date'][i]-WY_start\n",
    "        deltaweek = round(deltaday.days/7)\n",
    "        weeklist.append(deltaweek)\n",
    "\n",
    "\n",
    "    df['WYWeek'] = weeklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NaReplacement(region, RegionSnotel, prev_RegionSnotel):\n",
    "    \n",
    "    #Make NA values mean snowpack values\n",
    "    meanSWE = np.mean(RegionSnotel[region]['SWE'][RegionSnotel[region]['SWE']>0])\n",
    "    RegionSnotel[region]['SWE'][RegionSnotel[region]['SWE']<0]= meanSWE\n",
    "    \n",
    "    \n",
    "    prev_meanSWE = np.mean(prev_RegionSnotel[region]['SWE'][prev_RegionSnotel[region]['SWE']>0])\n",
    "    prev_RegionSnotel[region]['SWE'][prev_RegionSnotel[region]['SWE']<0]= prev_meanSWE\n",
    "    \n",
    "    \n",
    "\n",
    "    delta = RegionSnotel[region]['SWE']-prev_RegionSnotel[region]['SWE']\n",
    "    delta = pd.DataFrame(delta)\n",
    "    delta = delta.rename(columns = {'SWE':'Delta'})\n",
    "\n",
    "    #get values that are not affected by NA\n",
    "    delta = delta[delta['Delta']> -9000]\n",
    "\n",
    "    #Get mean Delta to adjust observed SWE\n",
    "    meanD = np.mean(delta['Delta'])\n",
    "\n",
    "    #go and fix current SWE observations\n",
    "    #Get bad obsevations and snotel sites\n",
    "    badSWE_df = RegionSnotel[region][RegionSnotel[region]['SWE']< 0].copy()\n",
    "    bad_sites = list(badSWE_df.index)\n",
    "\n",
    "\n",
    "    #remove bad observations from SWE obsevations\n",
    "    RegionSnotel[region] = RegionSnotel[region][RegionSnotel[region]['SWE'] >= 0]\n",
    "\n",
    "    #Fix bad observatoins by taking previous obs +/- mean delta SWE\n",
    " #   print('Fixing these bad sites in ', region, ':')\n",
    "  #  for badsite in bad_sites:\n",
    "   #     print(badsite)\n",
    "    #    badSWE_df.loc[badsite,'SWE']=prev_RegionSnotel[region].loc[badsite]['SWE'] + meanD\n",
    "\n",
    "    #Add observations back to DF\n",
    "    RegionSnotel[region] = pd.concat([RegionSnotel[region], badSWE_df])\n",
    "    \n",
    "    return  RegionSnotel[region]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-circle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up model prediction function\n",
    "def SWE_Initial_Predict(Forecast, Region, Region_optfeatures):\n",
    "    \n",
    "    #region specific features\n",
    "    features = Region_optfeatures[Region]\n",
    "    \n",
    "    #Make prediction dataframe\n",
    "    forecast_data = Forecast[Region].copy()\n",
    "    forecast_data = forecast_data[features]\n",
    "   \n",
    "    \n",
    "    #change all na values to prevent scaling issues\n",
    "    forecast_data[forecast_data< -9000]= -10\n",
    "\n",
    "    \n",
    "    #load and scale data\n",
    "    \n",
    "    #set up filepath to extract best model\n",
    "    checkpoint_filepath = 'Model/Initial_Models_Final/'+Region+ '/'\n",
    "    model = checkpoint_filepath+Region+'_model.h5'\n",
    "    print(model)\n",
    "    model=load_model(model)\n",
    "    \n",
    "    \n",
    "    #load SWE scaler\n",
    "    SWEmax = np.load(checkpoint_filepath+Region+'_SWEmax.npy')\n",
    "    SWEmax = SWEmax.item()\n",
    "    \n",
    "    #load features scaler\n",
    "    #save scaler data here too\n",
    "    scaler =  pickle.load(open(checkpoint_filepath+Region+'_scaler.pkl', 'rb'))\n",
    "    scaled = scaler.transform(forecast_data)\n",
    "    x_forecast = pd.DataFrame(scaled, columns = forecast_data.columns)\n",
    "   \n",
    "    \n",
    "    \n",
    "     #make predictions and rescale\n",
    "    y_forecast = (model.predict(x_forecast))\n",
    "\n",
    "    y_forecast[y_forecast < 0 ] = 0\n",
    "    y_forecast = (SWEmax * y_forecast)\n",
    "    Forecast[Region][datecol] = y_forecast\n",
    "    \n",
    "    \n",
    "    #plot predictions    \n",
    "    plt.scatter( Forecast[Region]['elevation_m'],Forecast[Region][datecol], s=5, color=\"blue\", label=\"Predictions\")\n",
    "    plt.xlabel('elevation m')\n",
    "    plt.ylabel('Predicted SWE')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    #plt.plot(x_ax, y_pred, lw=0.8, color=\"red\", label=\"predicted\")\n",
    "    plt.title(Region)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    #plot geolocation information\n",
    "    _geom = [Point(xy) for xy in zip(Forecast[Region]['Long'], Forecast[Region]['Lat'])]\n",
    "    _geom_df = gpd.GeoDataFrame(Forecast[Region], crs=\"EPSG:4326\", geometry=_geom)\n",
    "\n",
    "    dfmax = max(Forecast[Region][datecol])*1.05\n",
    "\n",
    "    # fig, ax = plt.subplots(figsize=(14,6))\n",
    "    ax = _geom_df.plot(datecol, cmap=\"cool\", markersize=30,figsize=(25,25), legend=True, vmin=0, vmax=dfmax)#vmax=test_preds['delta'].max(), vmin=test_preds['delta'].min())\n",
    "    cx.add_basemap(ax, alpha = .7, crs=_geom_df.crs.to_string())\n",
    "\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return Forecast[Region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-palestine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-horizon",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get Up to date snotel measurements and connect to validation DF/Regions\n",
    "\n",
    "This data assimilation codes makes the second week's observations\n",
    "and saves them to build on for the next set of observations (week 3-)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-treasure",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up training DF with key metadata per site\n",
    "#All coordinates of 1 km polygon used to develop ave elevation, ave slope, ave aspect\n",
    "path = 'Data/Processed/RegionVal.pkl'\n",
    "#load regionalized geospatial data\n",
    "RegionTest = open(path, \"rb\")\n",
    "RegionTest = pickle.load(RegionTest)\n",
    "\n",
    "\n",
    "#input current and previous weeks dates (these upload csv, must match dates)\n",
    "date = '01_20_2022'\n",
    "datekey = '1/20/2022'\n",
    "\n",
    "prevdate = '01_13_2022'\n",
    "prevdatekey = '1/13/2022'\n",
    "\n",
    "#load ground truth values (SNOTEL): Testing\n",
    "obs_path = 'Data/Pre_Processed/ground_measures_features_' + date + '.csv'\n",
    "GM_Test = pd.read_csv(obs_path)\n",
    "\n",
    "#change first column to station id\n",
    "GM_Test = GM_Test.rename(columns = {'Unnamed: 0':'station_id'})\n",
    "\n",
    "\n",
    "#Fill NA observations\n",
    "GM_Test[datekey] = GM_Test[datekey].fillna(-9999)\n",
    "\n",
    "#drop na and put into modeling df format\n",
    "GM_Test = GM_Test.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#change variable to Date and value to SWE\n",
    "GM_Test = GM_Test.rename(columns ={'variable': 'Date', 'value':'SWE'})\n",
    "\n",
    "#load ground truth meta\n",
    "GM_Meta = pd.read_csv('Data/Pre_Processed/ground_measures_metadata.csv')\n",
    "\n",
    "#merge testing ground truth location metadata with snotel data\n",
    "GM_Test = GM_Meta.merge(GM_Test, how='inner', on='station_id')\n",
    "GM_Test = GM_Test.set_index('station_id')\n",
    "GM_Test.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#load ground truth values (SNOTEL): previous week\n",
    "obs_path = 'Data/Pre_Processed/ground_measures_features_' + prevdate + '.csv'\n",
    "GM_Prev = pd.read_csv(obs_path)\n",
    "\n",
    "#change first column to station id\n",
    "GM_Prev = GM_Prev.rename(columns = {'Unnamed: 0':'station_id'})\n",
    "\n",
    "#Fill NA observations\n",
    "GM_Prev[prevdatekey] = GM_Prev[prevdatekey].fillna(-9999)\n",
    "\n",
    "#drop na and put into modeling df format\n",
    "GM_Prev = GM_Prev.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#change variable to Date and value to SWE\n",
    "GM_Prev = GM_Prev.rename(columns ={'variable': 'Date', 'value':'SWE'})\n",
    "\n",
    "#merge testing ground truth location metadata with snotel data\n",
    "GM_Prev = GM_Meta.merge(GM_Prev, how='inner', on='station_id')\n",
    "GM_Prev = GM_Prev.set_index('station_id')\n",
    "GM_Prev.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-diary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a dictionary for current snotel observations\n",
    "Snotel = GM_Test.copy()\n",
    "Snotel['Region'] = 'other'\n",
    "Region_id(Snotel)\n",
    "RegionSnotel  = {name: Snotel.loc[Snotel['Region'] == name] for name in Snotel.Region.unique()}\n",
    "\n",
    "#Make a dictionary for previous week's snotel observations\n",
    "prev_Snotel = GM_Prev.copy()\n",
    "prev_Snotel['Region'] = 'other'\n",
    "Region_id(prev_Snotel)\n",
    "prev_RegionSnotel  = {name: prev_Snotel.loc[prev_Snotel['Region'] == name] for name in prev_Snotel.Region.unique()}\n",
    "\n",
    "\n",
    "\n",
    "#add week number to observations\n",
    "for i in RegionTest.keys():\n",
    "    RegionTest[i] = RegionTest[i].reset_index(drop=True)\n",
    "    RegionTest[i]['Date'] = pd.to_datetime(RegionSnotel[i]['Date'][0])\n",
    "    week_num(RegionTest[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-upper",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##checking to make sure not bad features are used as key model features\n",
    "OG_Region_list = ['N_Sierras',\n",
    "               'S_Sierras',\n",
    "               'Greater_Yellowstone',\n",
    "               'N_Co_Rockies',\n",
    "               'SW_Mont',\n",
    "               'SW_Co_Rockies',\n",
    "               'GBasin',\n",
    "               'N_Wasatch',\n",
    "               'N_Cascade',\n",
    "               'S_Wasatch',\n",
    "               'SW_Mtns',\n",
    "               'E_WA_N_Id_W_Mont',\n",
    "               'S_Wyoming',\n",
    "               'SE_Co_Rockies',\n",
    "               'Sawtooth',\n",
    "               'Ca_Coast',\n",
    "               'E_Or',\n",
    "               'N_Yellowstone',\n",
    "               'S_Cascade',\n",
    "               'Wa_Coast',\n",
    "               'Greater_Glacier',\n",
    "               'Or_Coast'\n",
    "              ]\n",
    "\n",
    "#set up dataframe to save to be future GM_Pred\n",
    "col = list(GM_Test.columns)+['Region']\n",
    "Future_GM_Pred = pd.DataFrame(columns = col)\n",
    "\n",
    "for region in OG_Region_list:\n",
    "    RegionSnotel[region] = NaReplacement(region, RegionSnotel, prev_RegionSnotel)\n",
    "    RegionSnotel[region]['Prev_SWE'] =prev_RegionSnotel[region]['SWE']\n",
    "    RegionSnotel[region]['Delta_SWE'] = RegionSnotel[region]['SWE'] - RegionSnotel[region]['Prev_SWE']\n",
    "    \n",
    "    #make dataframe to function as next forecasts GM_Prev\n",
    "    Future_GM_Pred = Future_GM_Pred.append(RegionSnotel[region])\n",
    "    \n",
    "#Need to save 'updated non-na' df's\n",
    "GM_path = 'Data/Processed/DA_ground_measures_features_' + date + '.csv'\n",
    "\n",
    "Future_GM_Pred.to_csv(GM_path)\n",
    "\n",
    "#This needs to be here to run in next codeblock\n",
    "Regions = list(RegionTest.keys()).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaecf2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Make dictionary in Regions dict for each region's dictionary of Snotel sites\n",
    "#Regions = list(RegionTrain.keys()).copy()\n",
    "\n",
    "for i in tqdm(Regions):\n",
    "    \n",
    "    snotel = i+'_Snotel'\n",
    "    RegionTest[snotel] = {site: RegionSnotel[i].loc[site] for site in RegionSnotel[i].index.unique()}\n",
    "    \n",
    "   #get training and testing sites that are the same\n",
    "    test = RegionTest[snotel].keys()\n",
    "    \n",
    "    for j in test:\n",
    "        RegionTest[snotel][j] = RegionTest[snotel][j].to_frame().T\n",
    "    #remove items we do not need\n",
    "        RegionTest[snotel][j] = RegionTest[snotel][j].drop(columns = ['Long', 'Lat', 'location',\n",
    "                                                                      'elevation_m', 'state', 'Region'])\n",
    "    #make date index\n",
    "        RegionTest[snotel][j] = RegionTest[snotel][j].set_index('Date')\n",
    "        \n",
    "    #rename columns to represent site info\n",
    "        colnames = RegionTest[snotel][j].columns\n",
    "        sitecolnames = [x +'_'+ j for x in colnames]\n",
    "        names = dict(zip(colnames, sitecolnames))\n",
    "        RegionTest[snotel][j] = RegionTest[snotel][j].rename(columns = names)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a df for training each region, \n",
    "\n",
    "for R in tqdm(Regions):\n",
    "    snotels = R+'_Snotel'  \n",
    "   # RegionTest[R] = RegionTest[R].reset_index()\n",
    "   # print(R)\n",
    "    sites = list(RegionTest[R]['cell_id'])\n",
    "    sitelen = len(sites)-1\n",
    "    RegionTest[R] = RegionTest[R].set_index('cell_id')\n",
    "    \n",
    "    \n",
    "    for S in RegionTest[snotels].keys():\n",
    "       # print(S)\n",
    "        RegionTest[snotels][S] = RegionTest[snotels][S].append([RegionTest[snotels][S]]*sitelen, ignore_index = True)\n",
    "        RegionTest[snotels][S].index = sites\n",
    "        RegionTest[R]= pd.concat([RegionTest[R], RegionTest[snotels][S].reindex(RegionTest[R].index)], axis=1)\n",
    "    \n",
    "    RegionTest[R] = RegionTest[R].fillna(-9999.99)\n",
    "    del RegionTest[R]['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the splitting for S_Sierras High and Low elevations\n",
    "RegionTest['S_Sierras_High'] =RegionTest['S_Sierras'].loc[RegionTest['S_Sierras']['elevation_m'] > 2500].copy()\n",
    "RegionTest['S_Sierras_Low'] = RegionTest['S_Sierras'].loc[RegionTest['S_Sierras']['elevation_m'] <= 2500].copy()\n",
    "del RegionTest['S_Sierras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionaries as pkl\n",
    "# create a binary pickle file \n",
    "path = 'Data/Processed/ValidationDF_'+date + '.pkl'\n",
    "\n",
    "RVal = open(path,\"wb\")\n",
    "\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(RegionTest,RVal)\n",
    "\n",
    "\n",
    "# close file\n",
    "RVal.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-martin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "based-companion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-alexandria",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-yellow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-stevens",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Take in initial conditions Observations for use in an initial SWE prediction\n",
    "'''\n",
    "#load first SWE observation forecasting dataset with prev and delta swe for observations. \n",
    "\n",
    "date = '01_20_2022'\n",
    "datecol = '2022-01-20'\n",
    "\n",
    "\n",
    "path = 'Data/Processed/ValidationDF_'+date + '.pkl'\n",
    "\n",
    "\n",
    "#load regionalized forecast data\n",
    "Forecast = open(path, \"rb\")\n",
    "\n",
    "Forecast = pickle.load(Forecast)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#load RFE optimized features\n",
    "Region_optfeatures= pickle.load(open(\"Model/Initial_Models_Final/opt_features_initial.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### define new regions\n",
    "Region_list = ['N_Sierras',\n",
    "               'S_Sierras_High',\n",
    "               'S_Sierras_Low',\n",
    "               'Greater_Yellowstone',\n",
    "               'N_Co_Rockies',\n",
    "               'SW_Mont',\n",
    "               'SW_Co_Rockies',\n",
    "               'GBasin',\n",
    "               'N_Wasatch',\n",
    "               'N_Cascade',\n",
    "               'S_Wasatch',\n",
    "               'SW_Mtns',\n",
    "               'E_WA_N_Id_W_Mont',\n",
    "               'S_Wyoming',\n",
    "               'SE_Co_Rockies',\n",
    "               'Sawtooth',\n",
    "               'Ca_Coast',\n",
    "               'E_Or',\n",
    "               'N_Yellowstone',\n",
    "               'S_Cascade',\n",
    "               'Wa_Coast',\n",
    "               'Greater_Glacier',\n",
    "               'Or_Coast'\n",
    "              ]\n",
    "#Reorder regions\n",
    "Forecast = {k: Forecast[k] for k in Region_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-values",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#make and save predictions\n",
    "Prev_df = pd.DataFrame()\n",
    "predictions ={}\n",
    "for Region in Region_list:\n",
    "    print(Region)\n",
    "    predictions[Region] = SWE_Initial_Predict(Forecast, Region, Region_optfeatures)\n",
    "    predictions[Region] = pd.DataFrame(predictions[Region])\n",
    "    \n",
    "    del predictions[Region]['geometry']\n",
    "    \n",
    "    Prev_df = Prev_df.append(pd.DataFrame(predictions[Region][datecol]))\n",
    "    Prev_df = pd.DataFrame(Prev_df)\n",
    "    \n",
    "    \n",
    "    predictions[Region].to_hdf('Predictions/predictions'+datecol+'.h5', key = Region)\n",
    "    \n",
    "    \n",
    "#load submission DF and add predictions\n",
    "subdf = pd.read_csv('Predictions/submission_format.csv')\n",
    "subdf = subdf.rename(columns = {'Unnamed: 0':'cell_id'})\n",
    "subdf = subdf.set_index('cell_id')\n",
    "\n",
    "sub_index = subdf.index\n",
    "Prev_df = Prev_df.loc[sub_index]\n",
    "subdf[datecol] = Prev_df[datecol].astype(float)\n",
    "subdf['2022-01-13'] = subdf[datecol].astype(float)\n",
    "subdf.to_csv('Predictions/submission_format_'+datecol+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "general-simon",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
