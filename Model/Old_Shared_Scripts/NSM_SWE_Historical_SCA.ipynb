{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "boolean-pocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 14:44:46.298929: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-04-22 14:44:46.298951: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Run on virtual environment: Snow_env with the following packages installed\n",
    "***tested on Python 3.9.12, does not work with python 3.10***\n",
    "\n",
    "Load all dependencies via cmd and pay special attention to version requirements.\n",
    "\n",
    "Steps for installing virtual env: https://janakiev.com/blog/jupyter-virtual-envs/\n",
    "\n",
    "1st geopandas: https://towardsdatascience.com/geopandas-installation-the-easy-way-for-windows-31a666b3610f\n",
    "Here are binaries: https://www.lfd.uci.edu/~gohlke/pythonlibs/\n",
    "2nd: rioxarray\n",
    "3rd: rasterio\n",
    "tables\n",
    "sklearn\n",
    "earthpy\n",
    "numpy 1.22\n",
    "pyshp 2.1\n",
    "h5py\n",
    "tqdm\n",
    "matplotlib 3.5\n",
    "seaborn\n",
    "tensorflow\n",
    "\n",
    "\n",
    "\n",
    "joblib\n",
    "pillow\n",
    "panda\n",
    "pyproj\n",
    "shapely\n",
    "kiwisolver\n",
    "fonttools\n",
    "pillow\n",
    "mpl_toolkits - pip install --user basemap\n",
    "**git clone https://github.com/matplotlib/matplotlib\n",
    "cd matplotlib\n",
    "pip install -e .**\n",
    "tables\n",
    "pytables\n",
    "netCDF4 **https://unidata.github.io/netcdf4-python/\n",
    "\n",
    "Data Assimilation of snotel data\n",
    "https://snowex-hackweek.github.io/website/tutorials/geospatial/SNOTEL_query.html\n",
    "\n",
    "'''\n",
    "\n",
    "import os\n",
    "import National_Snow_Model\n",
    "import NSM_SCA\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from datetime import date, timedelta\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13048ad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/rjohnson18/SWEML'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set path directory\n",
    "os.getcwd()\n",
    "os.chdir('..')  # TODO find a way around this, it causes issues when rerunning the code\n",
    "cwd = os.getcwd()\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1216932",
   "metadata": {},
   "source": [
    "## Running the NSM to create retrospective datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72c30500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".h5 file complete\n",
      "Ground measures features df complete\n",
      "Ground measures features meta df complete\n",
      "New simulation start files complete\n"
     ]
    }
   ],
   "source": [
    "#Had to add folders and files manually - 4-22-2024\n",
    "\n",
    "#Grab existing files based on water year\n",
    "prev_year = '2022'\n",
    "prev_date = prev_year + '-09-24'\n",
    "\n",
    "#input the new water year of choice\n",
    "new_year = '2018'\n",
    "new_date = new_year + '-09-24'\n",
    "\n",
    "#threshold\n",
    "threshold = '20.0'\n",
    "\n",
    "#write code for CSV files\n",
    "\n",
    "#for h5 files\n",
    "Region_list = ['N_Sierras',\n",
    "                       'S_Sierras_High',\n",
    "                       'S_Sierras_Low',\n",
    "                       'Greater_Yellowstone',\n",
    "                       'N_Co_Rockies',\n",
    "                       'SW_Mont',\n",
    "                       'SW_Co_Rockies',\n",
    "                       'GBasin',\n",
    "                       'N_Wasatch',\n",
    "                       'N_Cascade',\n",
    "                       'S_Wasatch',\n",
    "                       'SW_Mtns',\n",
    "                       'E_WA_N_Id_W_Mont',\n",
    "                       'S_Wyoming',\n",
    "                       'SE_Co_Rockies',\n",
    "                       'Sawtooth',\n",
    "                       'Ca_Coast',\n",
    "                       'E_Or',\n",
    "                       'N_Yellowstone',\n",
    "                       'S_Cascade',\n",
    "                       'Wa_Coast',\n",
    "                       'Greater_Glacier',\n",
    "                       'Or_Coast'\n",
    "                      ]\n",
    "\n",
    "#dragged this file in from local/Box\n",
    "SWE_new = {}\n",
    "for region in Region_list:\n",
    "    #The below file will serve as a starting poinw\n",
    "    SWE_new[region] = pd.read_hdf(cwd+f\"/Predictions/predictions{prev_year}-09-24.h5\", key = region)\n",
    "    SWE_new[region].rename(columns = {prev_date:new_date}, inplace = True)\n",
    "    path = f\"{cwd}/Predictions/{threshold}\"\n",
    "    if os.path.exists(path) == False:\n",
    "        os.makedirs(path) \n",
    "    SWE_new[region].to_hdf(f\"{path}/predictions{new_year}-09-24.h5\", key = region)\n",
    "print('.h5 file complete')\n",
    "    \n",
    "#set the ground measures features DF    \n",
    "obs_old = pd.read_csv(cwd+f\"/Data/Pre_Processed_DA/ground_measures_features_{prev_year}-09-24.csv\")\n",
    "obs_old.rename(columns = {'Unnamed: 0':'station_id', prev_date:new_date}, inplace = True)\n",
    "obs_old.set_index('station_id', inplace = True)\n",
    "obs_old[new_date] = 0\n",
    "obs_old.to_csv(cwd+f\"/Data/Pre_Processed_DA/ground_measures_features_{new_year}-09-24.csv\")\n",
    "\n",
    "print('Ground measures features df complete')\n",
    "\n",
    "#load the ground_measures_features meta w/preds\n",
    "obs_meta_old = pd.read_csv(cwd+f\"/Data/Processed/DA_ground_measures_features_{prev_year}-09-24.csv\")\n",
    "obs_meta_old.rename(columns = {'Unnamed: 0':'station_id'}, inplace = True)\n",
    "obs_meta_old.set_index('station_id', inplace = True)\n",
    "obs_meta_old['Date'] = new_date\n",
    "obs_meta_old.to_csv(cwd+f\"/Data/Processed/DA_ground_measures_features_{new_year}-09-24.csv\")\n",
    "\n",
    "print('Ground measures features meta df complete')\n",
    "\n",
    "#Make a submission DF\n",
    "old_preds = pd.read_csv(cwd+ f\"/Predictions/submission_format_{prev_date}.csv\")\n",
    "old_preds['2022-10-01'] = 0\n",
    "new_preds_date = new_year+'-10-01'\n",
    "old_preds.rename(columns = {'2022-10-01':new_preds_date}, inplace = True)\n",
    "old_preds.set_index('cell_id', inplace = True)\n",
    "old_preds.to_csv(cwd+ f\"/Predictions/{threshold}/submission_format_{new_date}.csv\")\n",
    "print('New simulation start files complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3555dd51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2018-10-01',\n",
       " '2018-10-08',\n",
       " '2018-10-15',\n",
       " '2018-10-22',\n",
       " '2018-10-29',\n",
       " '2018-11-05',\n",
       " '2018-11-12',\n",
       " '2018-11-19',\n",
       " '2018-11-26',\n",
       " '2018-12-03',\n",
       " '2018-12-10',\n",
       " '2018-12-17',\n",
       " '2018-12-24',\n",
       " '2018-12-31',\n",
       " '2019-01-07',\n",
       " '2019-01-14',\n",
       " '2019-01-21',\n",
       " '2019-01-28',\n",
       " '2019-02-04',\n",
       " '2019-02-11',\n",
       " '2019-02-18',\n",
       " '2019-02-25',\n",
       " '2019-03-04',\n",
       " '2019-03-11',\n",
       " '2019-03-18',\n",
       " '2019-03-25',\n",
       " '2019-04-01',\n",
       " '2019-04-08',\n",
       " '2019-04-15',\n",
       " '2019-04-22',\n",
       " '2019-04-29',\n",
       " '2019-05-06',\n",
       " '2019-05-13',\n",
       " '2019-05-20',\n",
       " '2019-05-27',\n",
       " '2019-06-03',\n",
       " '2019-06-10',\n",
       " '2019-06-17',\n",
       " '2019-06-24',\n",
       " '2019-07-01',\n",
       " '2019-07-08',\n",
       " '2019-07-15',\n",
       " '2019-07-22',\n",
       " '2019-07-29']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#can be altered to create list every n number of days by changing 7 to desired skip length\n",
    "def daterange(start_date, end_date):\n",
    "     for n in range(0, int((end_date - start_date).days) + 1, 7):\n",
    "        yield start_date + timedelta(n)\n",
    "         \n",
    "#define start and end date for list of dates\n",
    "start_dt = date(2018, 10, 1)\n",
    "end_dt = date(2019, 7, 31)\n",
    "\n",
    "#create empty list to store dates\n",
    "datelist = []\n",
    "\n",
    "#append dates to list\n",
    "for dt in daterange(start_dt, end_dt):\n",
    "    #print(dt.strftime(\"%Y-%m-%d\"))\n",
    "    dt=dt.strftime('%Y-%m-%d')\n",
    "    datelist.append(dt)\n",
    "    \n",
    "datelist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea2cf6",
   "metadata": {},
   "source": [
    "## Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9386868b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating SWE predictions for  2018-10-01\n",
      "Missing 7 granules for 2018-09-28, downloading\n",
      "Order:  1\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://urs.earthdata.nasa.gov/oauth/authorize?app_type=401&client_id=PGVMJ5nUzSnQkI5o23gMxA&response_type=code&redirect_uri=https%3A%2F%2Fn5eil02u.ecs.nsidc.org%2FOPS%2Fredirect&state=aHR0cHM6Ly9uNWVpbDAydS5lY3MubnNpZGMub3JnL2VnaS9yZXF1ZXN0P3Nob3J0X25hbWU9Vk5QMTBBMUYmdmVyc2lvbj0yJnRlbXBvcmFsPTIwMTgtMDktMjhUMDAlM0EwMCUzQTAwWiUyQzIwMTgtMDktMjhUMjMlM0E1OSUzQTU5WiZwYWdlX3NpemU9MjAwMCZib3VuZGluZ19ib3g9LTEyMy4zNDA3ODUzMDk2MTQ4JTJDMzMuMzU4MjUzNzg2MzA0ODElMkMtMTA1LjA3ODAzNTU4MzQ2NDklMkM0OC45NzEwNjU3MDgwNzk2NSZwcm9qZWN0aW9uPUdFT0dSQVBISUMmZm9ybWF0PUdlb1RJRkYmYWdlbnQ9JmVtYWlsPXJqb2huc29uMTglNDB1YS5lZHUmQ292ZXJhZ2U9JTJGVklJUlNfR3JpZF9JTUdfMkQlMkZDR0ZfTkRTSV9Tbm93X0NvdmVyJnJlcXVlc3RfbW9kZT1hc3luYw",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m bbox \u001b[38;5;241m=\u001b[39m Snow\u001b[38;5;241m.\u001b[39mgetPredictionExtent()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#Initialize/Download the granules\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mSnow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializeGranules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSnow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSCA_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#Process observations into Model prediction ready format,\u001b[39;00m\n\u001b[1;32m     18\u001b[0m Snow\u001b[38;5;241m.\u001b[39mData_Processing()\n",
      "File \u001b[0;32m~/SWEML/Model/NSM_SCA.py:83\u001b[0m, in \u001b[0;36mNSM_SCA.initializeGranules\u001b[0;34m(self, bbox, dataFolder)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    Initalizes SCA information by fetching granules and merging them.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m        None - Initializes the following class variables: extentDF, granules, raster\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextentDF \u001b[38;5;241m=\u001b[39m calculateGranuleExtent(bbox, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelayedDate)  \u001b[38;5;66;03m# Get granule extent\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgranules \u001b[38;5;241m=\u001b[39m \u001b[43mfetchGranules\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataFolder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelayedDate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextentDF\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Fetch granules\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraster \u001b[38;5;241m=\u001b[39m createMergedRxr(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgranules[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/SWEML/Model/NSM_SCA.py:426\u001b[0m, in \u001b[0;36mfetchGranules\u001b[0;34m(boundingBox, dataFolder, date, extentDF, shouldDownload)\u001b[0m\n\u001b[1;32m    424\u001b[0m version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;241m>\u001b[39m datetime(\u001b[38;5;241m2018\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Use version 1 if date is before 2018\u001b[39;00m\n\u001b[1;32m    425\u001b[0m year \u001b[38;5;241m=\u001b[39m date\u001b[38;5;241m.\u001b[39myear \u001b[38;5;28;01mif\u001b[39;00m date\u001b[38;5;241m.\u001b[39mmonth \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m date\u001b[38;5;241m.\u001b[39myear \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Water years start on October 1st\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m \u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVNP10A1F\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemporal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataFolder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43myear\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mNASA\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masync\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    427\u001b[0m cells[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m cells\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: granuleFilepath(createGranuleGlobpath(dataFolder, date, x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m], x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m'\u001b[39m])),\n\u001b[1;32m    429\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    430\u001b[0m )  \u001b[38;5;66;03m# add filepath if it exists, otherwise add empty string\u001b[39;00m\n\u001b[1;32m    431\u001b[0m missingCells \u001b[38;5;241m=\u001b[39m cells[cells[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m][[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mto_dict(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecords\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/SWEML/Model/nsidc_fetch.py:233\u001b[0m, in \u001b[0;36mdownload\u001b[0;34m(short_name, version, temporal, bbox, folder, mode, format, subset)\u001b[0m\n\u001b[1;32m    228\u001b[0m request \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget(base_url, params\u001b[38;5;241m=\u001b[39mparam_dict, auth\u001b[38;5;241m=\u001b[39m(username, password))\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# print('Request HTTP response: ', request.status_code)\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# Raise bad request: Loop will stop for bad response code.\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m \u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# print('Order request URL: ', request.url)\u001b[39;00m\n\u001b[1;32m    235\u001b[0m esir_root \u001b[38;5;241m=\u001b[39m ET\u001b[38;5;241m.\u001b[39mfromstring(request\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[0;32m~/envs/SWEML_env/lib/python3.9/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://urs.earthdata.nasa.gov/oauth/authorize?app_type=401&client_id=PGVMJ5nUzSnQkI5o23gMxA&response_type=code&redirect_uri=https%3A%2F%2Fn5eil02u.ecs.nsidc.org%2FOPS%2Fredirect&state=aHR0cHM6Ly9uNWVpbDAydS5lY3MubnNpZGMub3JnL2VnaS9yZXF1ZXN0P3Nob3J0X25hbWU9Vk5QMTBBMUYmdmVyc2lvbj0yJnRlbXBvcmFsPTIwMTgtMDktMjhUMDAlM0EwMCUzQTAwWiUyQzIwMTgtMDktMjhUMjMlM0E1OSUzQTU5WiZwYWdlX3NpemU9MjAwMCZib3VuZGluZ19ib3g9LTEyMy4zNDA3ODUzMDk2MTQ4JTJDMzMuMzU4MjUzNzg2MzA0ODElMkMtMTA1LjA3ODAzNTU4MzQ2NDklMkM0OC45NzEwNjU3MDgwNzk2NSZwcm9qZWN0aW9uPUdFT0dSQVBISUMmZm9ybWF0PUdlb1RJRkYmYWdlbnQ9JmVtYWlsPXJqb2huc29uMTglNDB1YS5lZHUmQ292ZXJhZ2U9JTJGVklJUlNfR3JpZF9JTUdfMkQlMkZDR0ZfTkRTSV9Tbm93X0NvdmVyJnJlcXVlc3RfbW9kZT1hc3luYw"
     ]
    }
   ],
   "source": [
    "#run the model through all time (data acqusition already completed)\n",
    "for date in datelist:\n",
    "    print('Updating SWE predictions for ', date)\n",
    "    #connect interactive script to Wasatch Snow module\n",
    "    Snow = NSM_SCA.NSM_SCA(cwd, date, threshold=0.2)\n",
    "    \n",
    "    #Go get SNOTEL observations -- currently saving to csv, change to H5,\n",
    "    #dd if self.data < 11-1-2022 and SWE = -9999, \n",
    "    #Snow.Get_Monitoring_Data_Threaded()\n",
    "\n",
    "    #Get the prediction extent\n",
    "    bbox = Snow.getPredictionExtent()\n",
    "\n",
    "    #Initialize/Download the granules\n",
    "    Snow.initializeGranules(bbox, Snow.SCA_folder)\n",
    "\n",
    "    #Process observations into Model prediction ready format,\n",
    "    Snow.Data_Processing()\n",
    "\n",
    "    #Agument with SCA\n",
    "    Snow.augmentPredictionDFs()\n",
    "\n",
    "    #Make predictions\n",
    "    Snow.SWE_Predict()\n",
    "\n",
    "    #Make CONUS netCDF file, compressed.\n",
    "    #Snow.netCDF_compressed(plot=False)\n",
    "\n",
    "    #Make GeoDataframe and plot, self.Geo_df() makes the geo df\n",
    "    # Snow.Geo_df()\n",
    "    # Snow.plot_interactive_SWE_comp(pinlat = 39.1, pinlong = -120, web = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a54720cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Snow.threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e00ddf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(cwd, date):\n",
    "    print('Updating SWE predictions for ', date)\n",
    "    #connect interactive script to Wasatch Snow module\n",
    "    Snow = NSM_SCA.NSM_SCA(cwd, date)\n",
    "\n",
    "    #Go get SNOTEL observations -- currently saving to csv, change to H5,\n",
    "    #dd if self.data < 11-1-2022 and SWE = -9999,\n",
    "    Snow.Get_Monitoring_Data_Threaded()\n",
    "\n",
    "    #Get the prediction extent\n",
    "    bbox = Snow.getPredictionExtent()\n",
    "\n",
    "    #Initialize/Download the granules\n",
    "    Snow.initializeGranules(bbox, Snow.SCA_folder)\n",
    "\n",
    "    #Process observations into Model prediction ready format,\n",
    "    Snow.Data_Processing()\n",
    "\n",
    "    #Agument with SCA\n",
    "    Snow.augmentPredictionDFs()\n",
    "\n",
    "    #Make predictions\n",
    "    Snow.SWE_Predict()\n",
    "\n",
    "    #Make CONUS netCDF file, compressed.\n",
    "    Snow.netCDF_compressed(plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a9ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in datelist[:4]:\n",
    "    %time run(cwd, date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SWEML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
