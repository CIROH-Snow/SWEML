{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deluxe-phone",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n",
      "3.8.2\n"
     ]
    }
   ],
   "source": [
    "#Bring in elevation data\n",
    "import pandas as pd\n",
    "import json \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pystac_client import Client\n",
    "import planetary_computer\n",
    "import xarray\n",
    "import rioxarray\n",
    "from pyproj import Transformer\n",
    "import h5py\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import geopandas\n",
    "import richdem as rd\n",
    "import elevation\n",
    "import hdfdict\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "import io\n",
    "import os\n",
    "from datetime import date, timedelta\n",
    "\n",
    "import xarray as xr\n",
    "import requests\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cmocean\n",
    "\n",
    "# Not used directly, but used via xarray\n",
    "import cfgrib\n",
    "import netCDF4\n",
    "import tempfile\n",
    "import copy\n",
    "\n",
    "from matplotlib.axes import Axes\n",
    "from cartopy.mpl.geoaxes import GeoAxes\n",
    "GeoAxes._pcolormesh_patched = Axes.pcolormesh\n",
    "from platform import python_version\n",
    "\n",
    "# Ignore some matplotlib deprecation warnings\n",
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "print(pd.__version__) # should be 1.3.0\n",
    "print(python_version()) #should be 3.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "harmful-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added Long,Lat to get polygon points\n",
    "def GeoStat_func(Geospatial_df, regions, elev_L, slope_L, aspect_L, Long, Lat, tile):\n",
    "\n",
    "    #loop through\n",
    "    \n",
    "    for i in tqdm(range(0, len(Geospatial_df))):\n",
    "    \n",
    "        # convert coordinate to raster value\n",
    "        lon = Geospatial_df.iloc[i][Long]\n",
    "        lat = Geospatial_df.iloc[i][Lat]\n",
    "\n",
    "        #set up tile to speed up data processing\n",
    "        prev_tileid = 'NA'\n",
    "\n",
    "\n",
    "        #connect point location to geotile\n",
    "        tileid = 'Copernicus_DSM_COG_30_N' + str(math.floor(lat)) + '_00_W'+str(math.ceil(abs(lon))) +'_00_DEM'\n",
    "        \n",
    "        if tileid != prev_tileid:\n",
    "\n",
    "            indexid = regions.loc[tileid]['sliceID']\n",
    "\n",
    "\n",
    "           #Assing region\n",
    "            signed_asset = planetary_computer.sign(tiles[indexid].assets[\"data\"])\n",
    "            #get elevation data in xarray object\n",
    "            elevation = rioxarray.open_rasterio(signed_asset.href)\n",
    "            \n",
    "            prev_tileid = copy.copy(tileid)\n",
    "\n",
    "        #create copies to extract other geopysical information\n",
    "        #Create Duplicate DF's\n",
    "        slope = elevation.copy()\n",
    "        aspect = elevation.copy()\n",
    "\n",
    "\n",
    "        #transform projection\n",
    "        transformer = Transformer.from_crs(\"EPSG:4326\", elevation.rio.crs, always_xy=True)\n",
    "        xx, yy = transformer.transform(lon, lat)\n",
    "\n",
    "        #extract elevation values into numpy array\n",
    "        tilearray = np.around(elevation.values[0]).astype(int)\n",
    "\n",
    "        #set tile geo to get slope and set at rdarray\n",
    "        geo = (math.floor(float(lon)), 90, 0.0, math.ceil(float(lat)), 0.0, -90)\n",
    "        tilearray = rd.rdarray(tilearray, no_data = -9999)\n",
    "        tilearray.projection = 'EPSG:4326'\n",
    "        tilearray.geotransform = geo\n",
    "\n",
    "        #get slope, note that slope needs to be fixed, way too high\n",
    "        #get aspect value\n",
    "        slope_arr = rd.TerrainAttribute(tilearray, attrib='slope_degrees')\n",
    "        aspect_arr = rd.TerrainAttribute(tilearray, attrib='aspect')\n",
    "\n",
    "        #save slope and aspect information \n",
    "        slope.values[0] = slope_arr\n",
    "        aspect.values[0] = aspect_arr\n",
    "\n",
    "        # get point values from grid\n",
    "        elev = round(elevation.sel(x=(xx,), y=yy, method=\"nearest\").values[0][0])\n",
    "        slop = round(slope.sel(x=(xx,), y=yy, method=\"nearest\").values[0][0])\n",
    "        asp = round(aspect.sel(x=(xx,), y=yy, method=\"nearest\").values[0][0])\n",
    "\n",
    "\n",
    "        #add point values to list\n",
    "        elev_L.append(elev)\n",
    "        slope_L.append(slop)\n",
    "        aspect_L.append(asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mean Geospatial data\n",
    "def mean_Geo(df, geo):\n",
    "    BL = 'BL'+geo\n",
    "    UL = 'UL'+geo\n",
    "    UR = 'UR'+geo\n",
    "    BR = 'BR'+geo\n",
    "    \n",
    "    df[geo] = (df[BL] + df[UL]+ df[UR] + df[BR]) /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "after-archives",
   "metadata": {},
   "outputs": [],
   "source": [
    "def week_num(df):\n",
    "        #week of water year\n",
    "    weeklist = []\n",
    "\n",
    "    for i in tqdm(range(0,len(df))):\n",
    "        if df['Date'][i].month<11:\n",
    "            y = df['Date'][i].year-1\n",
    "        else:\n",
    "            y = df['Date'][i].year\n",
    "            \n",
    "        WY_start = pd.to_datetime(str(y)+'-10-01')\n",
    "        deltaday = df['Date'][i]-WY_start\n",
    "        deltaweek = round(deltaday.days/7)\n",
    "        weeklist.append(deltaweek)\n",
    "\n",
    "\n",
    "    df['WYWeek'] = weeklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "revised-consumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make Region identifier. The data already includes Region, but too many 'other' labels\n",
    "\n",
    "def Region_id(df):\n",
    "    \n",
    "    for i in tqdm(range(0, len(df))):\n",
    "\n",
    "        #Sierras\n",
    "        #Northern Sierras\n",
    "        if -122.5 <= df['Long'][i] <=-119 and 39 <=df['Lat'][i] <= 42:\n",
    "            loc = 'N_Sierras'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "        #Southern Sierras\n",
    "        if -121.2 <= df['Long'][i] <=-117 and 35 <=df['Lat'][i] <= 39:\n",
    "            loc = 'S_Sierras'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #West Coast    \n",
    "        #CACoastal (Ca-Or boarder)\n",
    "        if df['Long'][i] <=-122.5 and df['Lat'][i] <= 42:\n",
    "            loc = 'Ca_Coast'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Oregon Coastal (Or)?\n",
    "        if df['Long'][i] <=-122.7 and 42<= df['Lat'][i] <= 46:\n",
    "            loc = 'Or_Coast'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Olympis Coastal (Wa)\n",
    "        if df['Long'][i] <=-122.5 and 46<= df['Lat'][i]:\n",
    "            loc = 'Wa_Coast'\n",
    "            df['Region'].iloc[i] = loc    \n",
    "\n",
    "\n",
    "\n",
    "        #Cascades    \n",
    "         #Northern Cascades\n",
    "        if -122.5 <= df['Long'][i] <=-119.4 and 46 <=df['Lat'][i]:\n",
    "            loc = 'N_Cascade'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Southern Cascades\n",
    "        if -122.7 <= df['Long'][i] <=-121 and 42 <=df['Lat'][i] <= 46:\n",
    "            loc = 'S_Cascade'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Eastern Cascades and Northern Idaho and Western Montana\n",
    "        if -119.4 <= df['Long'][i] <=-116.4 and 46 <=df['Lat'][i]:\n",
    "            loc = 'E_WA_N_Id_W_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "        #Eastern Cascades and Northern Idaho and Western Montana\n",
    "        if -116.4 <= df['Long'][i] <=-114.1 and 46.6 <=df['Lat'][i]:\n",
    "            loc = 'E_WA_N_Id_W_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Eastern Oregon\n",
    "        if -121 <= df['Long'][i] <=-116.4 and 43.5 <=df['Lat'][i] <= 46:\n",
    "            loc = 'E_Or'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Great Basin\n",
    "        if -121 <= df['Long'][i] <=-112 and 42 <=df['Lat'][i] <= 43.5:\n",
    "            loc = 'GBasin'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "        if -119 <= df['Long'][i] <=-112 and 39 <=df['Lat'][i] <= 42:\n",
    "            loc = 'GBasin'\n",
    "            df['Region'].iloc[i] = loc\n",
    "            #note this section includes mojave too\n",
    "        if -117 <= df['Long'][i] <=-113.2 and df['Lat'][i] <= 39:\n",
    "            loc = 'GBasin'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "        #SW Mtns (Az and Nm)\n",
    "        if -113.2 <= df['Long'][i] <=-107 and df['Lat'][i] <= 37:\n",
    "            loc = 'SW_Mtns'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Southern Wasatch + Utah Desert Peaks\n",
    "        if -113.2 <= df['Long'][i] <=-109 and 37 <= df['Lat'][i] <= 39:\n",
    "            loc = 'S_Wasatch'\n",
    "            df['Region'].iloc[i] = loc\n",
    "        #Southern Wasatch + Utah Desert Peaks\n",
    "        if -112 <= df['Long'][i] <=-109 and 39 <= df['Lat'][i] <= 40:\n",
    "            loc = 'S_Wasatch'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #Northern Wasatch + Bear River Drainage\n",
    "        if -112 <= df['Long'][i] <=-109 and 40 <= df['Lat'][i] <= 42.5:\n",
    "            loc = 'N_Wasatch'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #YellowStone, Winds, Big horns\n",
    "        if -111 <= df['Long'][i] <=-106.5 and 42.5 <= df['Lat'][i] <= 45.8:\n",
    "            loc = 'Greater_Yellowstone'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "        #North of YellowStone to Boarder\n",
    "        if -112.5 <= df['Long'][i] <=-106.5 and 45.8 <= df['Lat'][i]:\n",
    "            loc = 'N_Yellowstone'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "         #SW Montana and nearby Idaho\n",
    "        if -112 <= df['Long'][i] <=-111 and 42.5 <= df['Lat'][i] <=45.8:\n",
    "            loc = 'SW_Mont'\n",
    "            df['Region'].iloc[i] = loc \n",
    "         #SW Montana and nearby Idaho\n",
    "        if -113 <= df['Long'][i] <=-112 and 43.5 <= df['Lat'][i] <=45.8:\n",
    "            loc = 'SW_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "        #SW Montana and nearby Idaho\n",
    "        if -113 <= df['Long'][i] <=-112.5 and 45.8 <= df['Lat'][i] <=46.6:\n",
    "            loc = 'SW_Mont'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "         #Sawtooths, Idaho\n",
    "        if -116.4 <= df['Long'][i] <=-113 and 43.5 <= df['Lat'][i] <=46.6:\n",
    "            loc = 'Sawtooth'\n",
    "            df['Region'].iloc[i] = loc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Greater Glacier\n",
    "        if -114.1 <= df['Long'][i] <=-112.5 and 46.6 <= df['Lat'][i]:\n",
    "            loc = 'Greater_Glacier'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "         #Southern Wyoming \n",
    "        if -109 <= df['Long'][i] <=-104.5 and 40.99 <= df['Lat'][i] <= 42.5 :\n",
    "            loc = 'S_Wyoming'\n",
    "            df['Region'].iloc[i] = loc \n",
    "        #Southern Wyoming\n",
    "        if -106.5 <= df['Long'][i] <=-104.5 and 42.5 <= df['Lat'][i] <= 43.2:\n",
    "            loc = 'S_Wyoming'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "         #Northern Colorado Rockies\n",
    "        if -109 <= df['Long'][i] <=-104.5 and 38.3 <= df['Lat'][i] <= 40.99:\n",
    "            loc = 'N_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "         #SW Colorado Rockies\n",
    "        if -109 <= df['Long'][i] <=-106 and 36.99 <= df['Lat'][i] <= 38.3:\n",
    "            loc = 'SW_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #SE Colorado Rockies + Northern New Mexico\n",
    "        if -106 <= df['Long'][i] <=-104.5 and 34 <= df['Lat'][i] <= 38.3:\n",
    "            loc = 'SE_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc  \n",
    "        #SE Colorado Rockies + Northern New Mexico\n",
    "        if -107 <= df['Long'][i] <=-106 and 34 <= df['Lat'][i] <= 36.99:\n",
    "            loc = 'SE_Co_Rockies'\n",
    "            df['Region'].iloc[i] = loc \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "olympic-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This plots the location of all df data points\n",
    "\n",
    "def GeoPlot(df):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(12, 10)\n",
    "\n",
    "    #merc also works for projection # Cylindrical Equal Area. https://matplotlib.org/basemap/api/basemap_api.html#module-mpl_toolkits.basemap\n",
    "\n",
    "    m = Basemap(projection='cea', \\\n",
    "                llcrnrlat=29, urcrnrlat=50, \\\n",
    "                llcrnrlon=-125, urcrnrlon=-104, \\\n",
    "                lat_ts=20, \\\n",
    "                resolution='c')\n",
    "\n",
    "    m.bluemarble(scale=2)   # full scale will be overkill\n",
    "    m.drawcoastlines(color='white', linewidth=0.2)  # add coastlines\n",
    "\n",
    "\n",
    "    # draw coastlines, meridians and parallels.\n",
    "    #m.drawcoastlines()\n",
    "    m.drawcountries()\n",
    "    m.drawstates()\n",
    "    #m.drawmapboundary(fill_color='#99ffff')\n",
    "    #m.fillcontinents(color='#cc9966',lake_color='#99ffff')\n",
    "    m.drawparallels(np.arange(20,60,10),labels=[1,1,0,0])\n",
    "    m.drawmeridians(np.arange(-120,-90,10),labels=[0,0,0,1])\n",
    "\n",
    "\n",
    "    #Make unique color for each regions\n",
    "    number_of_colors = len(df.keys())\n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(number_of_colors)]\n",
    "\n",
    "    Location = list(df.keys())\n",
    "    colordict = {k: v for k, v in zip(Location, color)}\n",
    "\n",
    "\n",
    "    for i in df.keys():\n",
    "            x, y = m(np.array(df[i]['Long']), np.array(df[i]['Lat'])) \n",
    "            m.scatter(x, y, 10, marker='o', color=colordict[i], label = str(i)) \n",
    "\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
    "\n",
    "    plt.title('Training Locations')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prompt-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function connects stationary geospatial information to observations\n",
    "def Geo_to_Data(geodf, SWE, id):\n",
    "    dfcols = ['Long','Lat','elevation_m','slope_deg','aspect','Date','SWE','Region']\n",
    "    datadf = geodf.merge(SWE, how='inner', on=id)\n",
    "    datadf = datadf.set_index(id)\n",
    "    datadf=datadf[dfcols]\n",
    "    return datadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "southwest-secret",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Attached new data (snotel) to regional training data\n",
    "def Region_Obs_Snotel(traindf, Snoteldf):\n",
    "    for i in traindf.keys():\n",
    "        traindf[i] = pd.concat([traindf[i], Snoteldf[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "innocent-mortgage",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#This function defines northness: :  sine(Slope) * cosine(Aspect). this gives you a northness range of -1 to 1.\n",
    "#Note you'll need to first convert to radians. \n",
    "#Some additional if else statements to get around sites with low obervations\n",
    "def northness(df):    \n",
    "    \n",
    "    if len(df) == 8: #This removes single value observations, need to go over and remove these locations from training too\n",
    "        #Determine northness for site\n",
    "        #convert to radians\n",
    "        #df = pd.DataFrame(df).T\n",
    "        \n",
    "        df['aspect_rad'] = df['aspect']*0.0174533\n",
    "        df['slope_rad'] = df['slope_deg']*0.0174533\n",
    "        \n",
    "        df['northness'] = -9999\n",
    "        for i in range(0, len(df)):\n",
    "            df['northness'].iloc[i] = math.sin(df['slope_rad'].iloc[i])*math.cos(df['aspect_rad'].iloc[i])\n",
    "\n",
    "        #remove slope and aspects to clean df up\n",
    "        df = df.drop(columns = ['aspect', 'slope_deg', 'aspect_rad', 'slope_rad', 'Region'])\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    else:\n",
    "         #convert to radians\n",
    "        df['aspect_rad'] = df['aspect']*0.0174533\n",
    "        df['slope_rad'] = df['slope_deg']*0.0174533\n",
    "        \n",
    "        df['northness'] = -9999\n",
    "        for i in range(0, len(df)):\n",
    "            df['northness'].iloc[i] = math.sin(df['slope_rad'].iloc[i])*math.cos(df['aspect_rad'].iloc[i])\n",
    "\n",
    "        \n",
    "         #remove slope and aspects to clean df up\n",
    "        df = df.drop(columns = ['aspect', 'slope_deg', 'aspect_rad', 'slope_rad', 'Region'])\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "expanded-masters",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/uufs/chpc.utah.edu/common/home/civil-group1/ItalianAlps/SnowCast/Final_Model'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certain-zoning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-jewelry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-mother",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adverse-founder",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set up training DF with key metadata per site\n",
    "#All coordinates of 1 km polygon used to develop ave elevation, ave slope, ave aspect\n",
    "\n",
    "colnames = ['cell_id', 'Region', 'BR_Coord', 'UR_Coord', 'UL_Coord', 'BL_Coord']\n",
    "SWEdata = pd.DataFrame(columns = colnames)\n",
    "\n",
    "#May or may not need to melt data\n",
    "#Load Testing SWE locations\n",
    "TestSWE = pd.read_csv('Data/Pre_Processed/submission_format_eval.csv')\n",
    "\n",
    "#change first column to cell id\n",
    "TestSWE = TestSWE.rename(columns = {'Unnamed: 0':'cell_id'})\n",
    "\n",
    "#drop na and put into modeling df format\n",
    "TestSWE = TestSWE.melt(id_vars=[\"cell_id\"]) #.dropna()\n",
    "\n",
    "#change variable to Date and value to SWE\n",
    "TestSWE = TestSWE.rename(columns ={'variable': 'Date', 'value':'SWE'})\n",
    "\n",
    "\n",
    "#Load  SWE location data\n",
    "with open(\"Data/Pre_Processed/grid_cells_eval.geojson\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "#load ground truth values (SNOTEL): Testing\n",
    "date = '01_27_2022'\n",
    "obs_path = 'Data/Pre_Processed/ground_measures_features_' + date + '.csv'\n",
    "GM_Test = pd.read_csv(obs_path)\n",
    "\n",
    "#change first column to station id\n",
    "GM_Test = GM_Test.rename(columns = {'Unnamed: 0':'station_id'})\n",
    "\n",
    "#drop na and put into modeling df format\n",
    "GM_Test = GM_Test.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#change variable to Date and value to SWE\n",
    "GM_Test = GM_Test.rename(columns ={'variable': 'Date', 'value':'SWE'})\n",
    "\n",
    "#load ground truth meta\n",
    "GM_Meta = pd.read_csv('Data/Pre_Processed/ground_measures_metadata.csv')\n",
    "\n",
    "#merge testing ground truth location metadata with snotel data\n",
    "GM_Test = GM_Meta.merge(GM_Test, how='inner', on='station_id')\n",
    "GM_Test = GM_Test.set_index('station_id')\n",
    "GM_Test.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#load ground truth values (SNOTEL): previous week\n",
    "date = '01_20_2022'\n",
    "obs_path = 'Data/Pre_Processed/ground_measures_features_' + date + '.csv'\n",
    "GM_Prev = pd.read_csv(obs_path)\n",
    "\n",
    "#change first column to station id\n",
    "GM_Prev = GM_Prev.rename(columns = {'Unnamed: 0':'station_id'})\n",
    "\n",
    "#drop na and put into modeling df format\n",
    "GM_Prev = GM_Prev.melt(id_vars=[\"station_id\"]).dropna()\n",
    "\n",
    "#change variable to Date and value to SWE\n",
    "GM_Prev = GM_Prev.rename(columns ={'variable': 'Date', 'value':'SWE'})\n",
    "\n",
    "#merge testing ground truth location metadata with snotel data\n",
    "GM_Prev = GM_Meta.merge(GM_Prev, how='inner', on='station_id')\n",
    "GM_Prev = GM_Prev.set_index('station_id')\n",
    "GM_Prev.rename(columns={'name': 'location', 'latitude': 'Lat', 'longitude': 'Long', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "#Add previous week's SWE to DF\n",
    "GM_Test['Prev_SWE'] = GM_Prev['SWE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "disciplinary-rough",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20759/20759 [01:14<00:00, 277.64it/s]\n"
     ]
    }
   ],
   "source": [
    "#Make a SWE Grid location DF\n",
    "for i in tqdm(range(len(data[\"features\"]))):\n",
    "    properties = data[\"features\"][i][\"properties\"]\n",
    "    location = data[\"features\"][i][\"geometry\"]\n",
    "    DFdata = [properties [\"cell_id\"],  properties [\"region\"],location [\"coordinates\"][0][0] ,\n",
    "             location [\"coordinates\"][0][1], location [\"coordinates\"][0][2], location [\"coordinates\"][0][3] ]\n",
    "    df_length = len(SWEdata)\n",
    "    SWEdata.loc[df_length] = DFdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "piano-giving",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make SWE location and observation DF\n",
    "#Testing\n",
    "#merge site location metadata with observations\n",
    "TestSWE = TestSWE.merge(SWEdata, how='inner', on='cell_id')\n",
    "TestSWE = TestSWE.set_index('cell_id')\n",
    "TestSWE.rename(columns={'variable': 'Date', 'value': 'SWE'}, inplace=True)\n",
    "\n",
    "#Get Lat Long information\n",
    "#Bottom right coord\n",
    "TestSWE[['BR_Coord_Long','BR_Coord_Lat']] = pd.DataFrame(TestSWE.BR_Coord.tolist(), index= TestSWE.index)\n",
    "\n",
    "#Upper right coord\n",
    "TestSWE[['UR_Coord_Long','UR_Coord_Lat']] = pd.DataFrame(TestSWE.UR_Coord.tolist(), index= TestSWE.index)\n",
    "\n",
    "#Upper left coord\n",
    "TestSWE[['UL_Coord_Long','UL_Coord_Lat']] = pd.DataFrame(TestSWE.UL_Coord.tolist(), index= TestSWE.index)\n",
    "\n",
    "#Bottom Left coord\n",
    "TestSWE[['BL_Coord_Long','BL_Coord_Lat']] = pd.DataFrame(TestSWE.BL_Coord.tolist(), index= TestSWE.index)\n",
    "\n",
    "#Get Lat Long information\n",
    "#TestSWE[['Long','Lat']] = pd.DataFrame(TestSWE.Lat_Long.tolist(), index= TestSWE.index)\n",
    "\n",
    "#recolumn df to increase interpretability\n",
    "testcols = TestSWE.columns.to_list()\n",
    "testcols = testcols[-13:] + testcols[:-13]\n",
    "TestSWE = TestSWE[testcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bigger-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up a framework to retrieve geospatial information for each site (elevation, weather, slope, aspect, etc)\n",
    "\n",
    "#Develop a DF to get each site's geospatial information \n",
    "geocols = [ 'BR_Coord_Long', 'BR_Coord_Lat', 'UR_Coord_Long', 'UR_Coord_Lat',\n",
    "       'UL_Coord_Long', 'UL_Coord_Lat', 'BL_Coord_Long', 'BL_Coord_Lat']\n",
    "\n",
    "\n",
    "Geospatial_df = TestSWE.copy()\n",
    "Geospatial_df['rowid'] = Geospatial_df.index\n",
    "Geospatial_df = Geospatial_df.drop_duplicates(subset = 'rowid')\n",
    "Geospatial_df = pd.DataFrame(Geospatial_df[geocols])\n",
    "\n",
    "#Define the AOI around the cell locations from clockwise\n",
    "\n",
    "area_of_interest = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            #lower left\n",
    "            [Geospatial_df['BL_Coord_Long'].min(), Geospatial_df['BL_Coord_Lat'].min()],\n",
    "            #upper left\n",
    "            [Geospatial_df['UL_Coord_Long'].min(), Geospatial_df['UL_Coord_Lat'].max()],\n",
    "            #upper right\n",
    "            [Geospatial_df['UR_Coord_Long'].max(), Geospatial_df['UR_Coord_Lat'].max()],\n",
    "            #lower right\n",
    "            [Geospatial_df['UR_Coord_Long'].max(), Geospatial_df['BR_Coord_Lat'].min()],\n",
    "            #lower left\n",
    "            [Geospatial_df['BL_Coord_Long'].min(), Geospatial_df['BL_Coord_Lat'].min()],\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "\n",
    "#Make a connection to get 90m Copernicus Digital Elevation Model (DEM) data with the Planetary Computer STAC API\n",
    "\n",
    "client = Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    ignore_conformance=True,\n",
    ")\n",
    "\n",
    "\n",
    "search = client.search(\n",
    "    collections=[\"cop-dem-glo-90\"],\n",
    "    intersects=area_of_interest\n",
    ")\n",
    "\n",
    "tiles = list(search.get_items())\n",
    "\n",
    "#Make a DF to connect locations with the larger data tile, and then extract elevations\n",
    "regions = []\n",
    "\n",
    "for i in range(0, len(tiles)):\n",
    "    row = [i, tiles[i].id]\n",
    "    regions.append(row)\n",
    "regions = pd.DataFrame(columns = ['sliceID', 'tileID'], data = regions)\n",
    "regions = regions.set_index(regions['tileID'])\n",
    "del regions['tileID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "identified-allen",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20759/20759 [3:10:39<00:00,  1.81it/s]  \n"
     ]
    }
   ],
   "source": [
    "#Get geospatial information for each evaluation cell corner\n",
    "\n",
    "BLelev_L = []\n",
    "BLslope_L = []\n",
    "BLaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "GeoStat_func(Geospatial_df, regions, BLelev_L, BLslope_L, BLaspect_L, 'BL_Coord_Long', 'BL_Coord_Lat', tiles)\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['BL_Elevation_m'] = BLelev_L\n",
    "Geospatial_df['BL_slope_Deg'] = BLslope_L\n",
    "Geospatial_df['BLaspect_L'] = BLaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "needed-development",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20759/20759 [2:56:23<00:00,  1.96it/s]  \n"
     ]
    }
   ],
   "source": [
    "ULelev_L = []\n",
    "ULslope_L = []\n",
    "ULaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "GeoStat_func(Geospatial_df, regions, ULelev_L, ULslope_L, ULaspect_L,'UL_Coord_Long', 'UL_Coord_Lat', tiles) \n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['UL_Elevation_m'] = ULelev_L\n",
    "Geospatial_df['UL_slope_Deg'] = ULslope_L\n",
    "Geospatial_df['ULaspect_L'] = ULaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-mailman",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 15555/20759 [2:09:53<41:32,  2.09it/s]  "
     ]
    }
   ],
   "source": [
    "URelev_L = []\n",
    "URslope_L = []\n",
    "URaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "GeoStat_func(Geospatial_df, regions, URelev_L, URslope_L, URaspect_L,'UR_Coord_Long', 'UR_Coord_Lat', tiles)\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['UR_Elevation_m'] = URelev_L\n",
    "Geospatial_df['UR_slope_Deg'] = URslope_L\n",
    "Geospatial_df['URaspect_L'] = URaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "BRelev_L = []\n",
    "BRslope_L = []\n",
    "BRaspect_L = []\n",
    "\n",
    "#run the elevation function, added tqdm to show progress\n",
    "GeoStat_func(Geospatial_df, regions, BRelev_L, BRslope_L, BRaspect_L,'BR_Coord_Long', 'BR_Coord_Lat', tiles)\n",
    "\n",
    "\n",
    "#Save each points elevation in DF\n",
    "Geospatial_df['BR_Elevation_m'] = BRelev_L\n",
    "Geospatial_df['BR_slope_Deg'] = BRslope_L\n",
    "Geospatial_df['BRaspect_L'] = BRaspect_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Geospatial data into SWE.h5 file\n",
    "Geospatial_df.to_hdf('Data/Pre_Processed/SWE.h5', key = 'Geospatial_Eval_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-amplifier",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reset index to make cell id a column name\n",
    "Geospatial_df = Geospatial_df.reset_index()\n",
    "\n",
    "#Get geaspatial means\n",
    "geospatialcols = ['_Coord_Long', '_Coord_Lat', '_Elevation_m', '_slope_Deg' , 'aspect_L']\n",
    "\n",
    "#Training data\n",
    "[mean_Geo(Geospatial_df, i) for i in geospatialcols]\n",
    "\n",
    "#list of key geospatial component means\n",
    "geocol = ['cell_id', '_Coord_Long','_Coord_Lat','_Elevation_m','_slope_Deg','aspect_L']\n",
    "TestGeo_df = Geospatial_df[geocol].copy()\n",
    "\n",
    "#adjust column names to be consistent with snotel\n",
    "\n",
    "TestGeo_df = TestGeo_df.rename( columns = {'_Coord_Long':'Long', '_Coord_Lat':'Lat', '_Elevation_m': 'elevation_m',\n",
    "                               '_slope_Deg':'slope_deg' , 'aspect_L': 'aspect'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-bumper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attach a region id for each location\n",
    "TestGeo_df['Region'] = 'other'\n",
    "Region_id(TestGeo_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Slice into regional DF's\n",
    "This is currently to ensure different regions are correctly classified. Will have to perform slice again\n",
    "\n",
    "'''\n",
    "#subset data by each region into dictionary\n",
    "RegionTest = {name: TestGeo_df.loc[TestGeo_df['Region'] == name] for name in TestGeo_df.Region.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-writer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#check to make sure no test locations classified as other\n",
    "print('Testing') \n",
    "for i in RegionTest.keys():\n",
    "    print('There are', len(RegionTest[i]), ' test locations in ', i)\n",
    "print('         ') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RegionTest should be the RegionVal. make sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-swiss",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "detailed-committee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save dictionaries as pkl\n",
    "# create a binary pickle file \n",
    "path = 'Data/Processed/RegionVal.pkl'\n",
    "\n",
    "RVal = open(path,\"wb\")\n",
    "\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(RegionTest,RVal)\n",
    "\n",
    "\n",
    "# close file\n",
    "RVal.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-aspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-oriental",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
